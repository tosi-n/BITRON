{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitbitronvirtualenv06ea2e62e7194357a4ffeeabcadac8df",
   "display_name": "Python 3.6.8 64-bit ('bitron': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: tokenizers==0.5.2 in ./bitron/lib/python3.6/site-packages (0.5.2)\nRequirement already satisfied: simpletransformers in ./bitron/lib/python3.6/site-packages (0.23.3)\nRequirement already satisfied: requests in ./bitron/lib/python3.6/site-packages (from simpletransformers) (2.23.0)\nRequirement already satisfied: scipy in ./bitron/lib/python3.6/site-packages (from simpletransformers) (1.4.1)\nRequirement already satisfied: numpy in ./bitron/lib/python3.6/site-packages (from simpletransformers) (1.18.2)\nRequirement already satisfied: tqdm in ./bitron/lib/python3.6/site-packages (from simpletransformers) (4.45.0)\nRequirement already satisfied: scikit-learn in ./bitron/lib/python3.6/site-packages (from simpletransformers) (0.22.2.post1)\nRequirement already satisfied: transformers in ./bitron/lib/python3.6/site-packages (from simpletransformers) (2.7.0)\nRequirement already satisfied: tokenizers in ./bitron/lib/python3.6/site-packages (from simpletransformers) (0.5.2)\nRequirement already satisfied: pandas in ./bitron/lib/python3.6/site-packages (from simpletransformers) (1.0.3)\nRequirement already satisfied: seqeval in ./bitron/lib/python3.6/site-packages (from simpletransformers) (0.0.12)\nRequirement already satisfied: regex in ./bitron/lib/python3.6/site-packages (from simpletransformers) (2020.4.4)\nRequirement already satisfied: tensorboardx in ./bitron/lib/python3.6/site-packages (from simpletransformers) (2.0)\nRequirement already satisfied: chardet<4,>=3.0.2 in ./bitron/lib/python3.6/site-packages (from requests->simpletransformers) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in ./bitron/lib/python3.6/site-packages (from requests->simpletransformers) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./bitron/lib/python3.6/site-packages (from requests->simpletransformers) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in ./bitron/lib/python3.6/site-packages (from requests->simpletransformers) (2020.4.5.1)\nRequirement already satisfied: joblib>=0.11 in ./bitron/lib/python3.6/site-packages (from scikit-learn->simpletransformers) (0.14.1)\nRequirement already satisfied: filelock in ./bitron/lib/python3.6/site-packages (from transformers->simpletransformers) (3.0.12)\nRequirement already satisfied: boto3 in ./bitron/lib/python3.6/site-packages (from transformers->simpletransformers) (1.12.36)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in ./bitron/lib/python3.6/site-packages (from transformers->simpletransformers) (0.7)\nRequirement already satisfied: sentencepiece in ./bitron/lib/python3.6/site-packages (from transformers->simpletransformers) (0.1.85)\nRequirement already satisfied: sacremoses in ./bitron/lib/python3.6/site-packages (from transformers->simpletransformers) (0.0.38)\nRequirement already satisfied: python-dateutil>=2.6.1 in ./bitron/lib/python3.6/site-packages (from pandas->simpletransformers) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in ./bitron/lib/python3.6/site-packages (from pandas->simpletransformers) (2019.3)\nRequirement already satisfied: Keras>=2.2.4 in ./bitron/lib/python3.6/site-packages (from seqeval->simpletransformers) (2.3.1)\nRequirement already satisfied: six in ./bitron/lib/python3.6/site-packages (from tensorboardx->simpletransformers) (1.14.0)\nRequirement already satisfied: protobuf>=3.8.0 in ./bitron/lib/python3.6/site-packages (from tensorboardx->simpletransformers) (3.11.3)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in ./bitron/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.3.3)\nRequirement already satisfied: botocore<1.16.0,>=1.15.36 in ./bitron/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (1.15.36)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./bitron/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.9.5)\nRequirement already satisfied: click in ./bitron/lib/python3.6/site-packages (from sacremoses->transformers->simpletransformers) (7.1.1)\nRequirement already satisfied: pyyaml in ./bitron/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (5.3.1)\nRequirement already satisfied: h5py in ./bitron/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\nRequirement already satisfied: keras-applications>=1.0.6 in ./bitron/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in ./bitron/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.0)\nRequirement already satisfied: setuptools in ./bitron/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (46.1.3)\nRequirement already satisfied: docutils<0.16,>=0.10 in ./bitron/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.36->boto3->transformers->simpletransformers) (0.15.2)\n"
    }
   ],
   "source": [
    "# !virtualenv -p python3 bitron \n",
    "!pip3 install tokenizers==0.5.2\n",
    "!pip3 install simpletransformers\n",
    "try:\n",
    "  from apex import amp\n",
    "except Exception:\n",
    "  ! git clone https://github.com/NVIDIA/apex.git\n",
    "  %cd apex\n",
    "  !pip install -v --no-cache-dir ./\n",
    "  %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:filelock:Lock 5228088232 acquired on /Users/tosi-n/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e.lock\nDownloading: 100%|██████████| 361/361 [00:00<00:00, 57.6kB/s]\nINFO:filelock:Lock 5228088232 released on /Users/tosi-n/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.3d5adf10d3445c36ce131f4c6416aa62e9b58e1af56b97664773f4858a46286e.lock\nINFO:filelock:Lock 5228087560 acquired on /Users/tosi-n/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\nDownloading: 100%|██████████| 436M/436M [57:24<00:00, 127kB/s]\nINFO:filelock:Lock 5228087560 released on /Users/tosi-n/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\nINFO:filelock:Lock 5205634184 acquired on /Users/tosi-n/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\nDownloading: 100%|██████████| 213k/213k [00:00<00:00, 261kB/s]\nINFO:filelock:Lock 5205634184 released on /Users/tosi-n/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\nINFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n100%|██████████| 1050/1050 [00:01<00:00, 537.59it/s]\nEpoch:   0%|          | 0/1 [00:00<?, ?it/s]\nRunning loss: 0.629609\nRunning loss: 0.706959\nRunning loss: 0.743938\nRunning loss: 0.674917\nRunning loss: 0.620816\nRunning loss: 0.686669\nRunning loss: 0.650615\nRunning loss: 0.605075\nRunning loss: 0.659064\nRunning loss: 0.614381\nRunning loss: 0.538492\nRunning loss: 0.508127\nRunning loss: 0.500640\nRunning loss: 0.365890\nRunning loss: 0.339106\nRunning loss: 0.349958\nRunning loss: 0.349960\nRunning loss: 0.289089\nRunning loss: 0.307276\nRunning loss: 0.233510\nRunning loss: 0.262252\nRunning loss: 0.176159\nRunning loss: 0.155268\nRunning loss: 0.130569\nRunning loss: 0.125758\nRunning loss: 0.135565\nRunning loss: 0.111210\nRunning loss: 0.091735\nRunning loss: 0.076953\nRunning loss: 0.072981\nRunning loss: 0.050264\nRunning loss: 0.041755\nRunning loss: 0.028499\nRunning loss: 0.018535\nRunning loss: 0.034743\nRunning loss: 0.029232\nRunning loss: 0.015058\nRunning loss: 0.008975\nRunning loss: 0.009933\nRunning loss: 0.004613\nRunning loss: 0.006894\nRunning loss: 0.004846\nRunning loss: 0.004226\nRunning loss: 0.004662\nRunning loss: 0.004530\nRunning loss: 0.003613\nRunning loss: 0.002578\nRunning loss: 0.002457\nRunning loss: 0.005096\nRunning loss: 0.003713\nRunning loss: 0.002568\nRunning loss: 0.002064\nRunning loss: 0.001258\nRunning loss: 0.001748\nRunning loss: 0.001260\nRunning loss: 0.002692\nRunning loss: 0.001268\nRunning loss: 0.001243\nRunning loss: 0.000886\nRunning loss: 0.738983\nRunning loss: 0.702768\nRunning loss: 0.001390\nRunning loss: 0.001615\nRunning loss: 0.001334\nRunning loss: 0.003429\nRunning loss: 0.002784\nRunning loss: 0.003665\nRunning loss: 0.003382\nRunning loss: 0.002704\nRunning loss: 0.005641\nRunning loss: 0.002515\nRunning loss: 0.001483\nRunning loss: 0.001781\nRunning loss: 0.001536\nRunning loss: 0.001082\nRunning loss: 0.001165\nRunning loss: 0.001129\nRunning loss: 0.000870\nRunning loss: 0.001601\nRunning loss: 0.000629\nRunning loss: 0.000678\nRunning loss: 0.000670\nRunning loss: 0.000578\nRunning loss: 0.000600\nRunning loss: 0.000557\nRunning loss: 0.000744\nRunning loss: 0.000701\nRunning loss: 0.001183\nRunning loss: 0.000628\nRunning loss: 0.000581\nRunning loss: 0.000544\nRunning loss: 0.000563\nRunning loss: 0.000559\nRunning loss: 0.000664\nRunning loss: 0.000539\nRunning loss: 0.000623\nRunning loss: 0.000649\nRunning loss: 0.000564\nRunning loss: 0.000609\nRunning loss: 0.000803\nRunning loss: 0.000501\nRunning loss: 0.000589\nRunning loss: 0.000625\nRunning loss: 0.000639\nRunning loss: 0.000493\nRunning loss: 0.000480\nRunning loss: 0.000578\nRunning loss: 0.000589\nRunning loss: 0.000489\nRunning loss: 0.000548\nRunning loss: 0.000508\nRunning loss: 0.000488\nRunning loss: 0.000460\nRunning loss: 0.000498\nRunning loss: 0.000584\nRunning loss: 0.000570\nRunning loss: 0.000702\nRunning loss: 0.000531\nRunning loss: 0.000653\nRunning loss: 0.000581\nRunning loss: 0.000571\nRunning loss: 0.000478\nRunning loss: 0.000392\nRunning loss: 0.000589\nRunning loss: 0.000429\nRunning loss: 0.000430\nRunning loss: 0.000399\nRunning loss: 0.000492\nRunning loss: 0.000429\nRunning loss: 0.000470\nRunning loss: 0.000347\nRunning loss: 0.000336\nCurrent iteration: 100%|██████████| 132/132 [19:15<00:00,  8.76s/it]\nEpoch: 100%|██████████| 1/1 [19:22<00:00, 1162.99s/it]\nINFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to outputs/.\nINFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n100%|██████████| 450/450 [00:01<00:00, 273.99it/s]\n100%|██████████| 57/57 [01:43<00:00,  1.81s/it]\nINFO:simpletransformers.classification.classification_model:{'mcc': 0.9899319849650976, 'tp': 147, 'tn': 301, 'fp': 1, 'fn': 1, 'eval_loss': 0.014266477926867083}\n"
    }
   ],
   "source": [
    "from classification import ClassificationModel\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sklearn\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int.\n",
    "\n",
    "# train_data = [['Example sentence belonging to class 1', 1], ['Example sentence belonging to class 0', 0]]\n",
    "# train_df = pd.DataFrame(train_data)\n",
    "train_df = pd.read_csv('/Volumes/Loopdisk/Bi_Transformer/data/train.csv', sep='\\t')\n",
    "\n",
    "train_df = train_df[['0', '1']]\n",
    "\n",
    "train_df['0'] = train_df['0'].astype(str)\n",
    "\n",
    "# eval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0]]\n",
    "# eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "eval_df = pd.read_csv('/Volumes/Loopdisk/Bi_Transformer/data/dev.csv', sep='\\t')\n",
    "\n",
    "eval_df = eval_df[['0', '1']]\n",
    "\n",
    "eval_df['0'] = eval_df['0'].astype(str)\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\"bert\", \"bert-base-cased\", use_cuda=False) # You can set class weights by using the optional weight argument\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n100%|██████████| 450/450 [00:02<00:00, 205.47it/s]\n100%|██████████| 57/57 [01:52<00:00,  1.97s/it]\nINFO:simpletransformers.classification.classification_model:{'mcc': 0.9899319849650976, 'tp': 147, 'tn': 301, 'fp': 1, 'fn': 1, 'acc': 0.9955555555555555, 'eval_loss': 0.014266477926867083}\n"
    }
   ],
   "source": [
    "model = ClassificationModel(\"bert\", \"outputs\", use_cuda=False)\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=sklearn.metrics.accuracy_score)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n100%|██████████| 1/1 [00:00<00:00, 20.98it/s]\n100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n Class: Eligibility Criteria\n\n"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\"Inclusion criteria:  Histologically or cytologically confirmed solid tumor that is metastatic or unresectable, and for which standard curative or palliative measures do not exist or are no longer effective, and there are no known therapies to prolong survival. Before any study-specific procedure, the appropriate Institutional Review Board (IRB) approved written informed consent must be obtained. Second informed consent must be obtained before the patient starts the Treatment Extension Period (Cycle 2 and after).  Measurable disease as per Response Evaluation Criteria in Solid Tumors (RECIST) criteria.  Exclusion criteria:  < 20 years old. Eastern Cooperative Oncology Group (ECOG) performance status > 2. Incapable of understanding or complying with the protocol or has not signed the informed consent document. Unable or unwilling to abide by the study protocol or cooperate fully with the investigator or designee. Inadequate organ or bone marrow function. Prothrombin time (PT)/International Normalized Ratio (INR) and/or partial thromboplastin time (PTT) test results at screening that are above 1.3 × the laboratory upper limit of normal (ULN). Baseline corrected QT interval (QTc) > 460 ms. Sexually active (males and females) who do not agree to use medically acceptable methods of contraception during the course of the study and for 3 months following discontinuation of study drug. Female patients of childbearing potential must have a negative pregnancy test at screening. Pregnant or breastfeeding. Has not tolerated previous treatment with other phosphatidylinositol 3-kinase (PI3K) inhibitor, or has been treated with SAR245408. Not recovered from all previous therapies (i.e. radiation, surgery, or medication) Currently receiving anticoagulation with therapeutic doses of warfarin (low-dose warfarin ≤ 1mg/day is permitted). Primary brain tumor or brain metastasis are considered eligible if the patient has not received radiation therapy for brain metastasis within 2 weeks of enrollment and has been on a stable dose of steroids for 2 or more weeks. Uncontrolled intercurrent illness including, but not limited to, ongoing or active infection (including cytomegalovirus, Epstein-Barr virus, toxoplasmosis, and hepatitis B and C), symptomatic congestive heart failure, unstable angina pectoris, or cardiac arrhythmia. Known to be positive for the human immunodeficiency virus (HIV) Psychiatric illness/social situation(s) that would limit compliance with study requirements. Allergy or hypersensitivity to components of the SAR245408 formulation. Withdraws consent during the screening (starting from signed informed consent form (ICF)) Patient who is judged by the investigator as not suitable for participating in the study  The above information is not intended to contain all considerations relevant to a patient's potential participation in a clinical trial.\"])\n",
    "\n",
    "\n",
    "if predictions == 1:s\n",
    "  print(\"\\n Class: Eligibility Criteria\")\n",
    "else: \n",
    "    print(\"\\n Class: Others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\n100%|██████████| 1/1 [00:00<00:00, 37.96it/s]\n100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n Class: Others\n\n"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\"Covid-19 is a new corona strain\"])\n",
    "if predictions == 1:\n",
    "  print(\"\\n Class: Eligibility Criteria\")\n",
    "else: \n",
    "    print(\"\\n Class: Others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}